{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Object Detection Tutorial {#vision__detection_tutorial}\n=========================\n\nIn this tutorial, you will learn how to validate your **object detection\nmodel** using deepchecks test suites. You can read more about the\ndifferent checks and suites for computer vision use cases at the\n`examples section <vision__checks_gallery>`{.interpreted-text\nrole=\"ref\"}.\n\nIf you just want to see the output of this tutorial, jump to the\n`observing the results <vision_segmentation_tutorial__observing_the_result>`{.interpreted-text\nrole=\"ref\"} section.\n\nAn object detection tasks usually consist of two parts:\n\n-   Object Localization, where the model predicts the location of an\n    object in the image,\n-   Object Classification, where the model predicts the class of the\n    detected object.\n\nThe common output of an object detection model is a list of bounding\nboxes around the objects, and their classes.\n\n``` {.bash}\n# Before we start, if you don't have deepchecks vision package installed yet, run:\nimport sys\n!{sys.executable} -m pip install \"deepchecks[vision]\" --quiet --upgrade # --user\n\n# or install using pip from your python environment\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the data and model\n===========================\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nIn this tutorial, we use the pytorch to create the dataset and model. To\nsee how this can be done using tensorflow or other frameworks, please\nvisit the `vision__vision_data_class`{.interpreted-text role=\"ref\"}\nguide.\n:::\n\nLoad Data\n---------\n\nThe model in this tutorial is used to detect tomatoes in images. The\nmodel is trained on a dataset consisted of 895 images of tomatoes, with\nbounding box annotations provided in PASCAL VOC format. All annotations\nbelong to a single class: tomato.\n\n::: {.note}\n::: {.title}\nNote\n:::\n\nThe dataset is available at the following link:\n<https://www.kaggle.com/andrewmvd/tomato-detection>\n\nWe thank the authors of the dataset for providing the dataset.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom PIL import Image\nimport xml.etree.ElementTree as ET\nimport urllib.request\nimport zipfile\n\nurl = 'https://figshare.com/ndownloader/files/34488599'\nurllib.request.urlretrieve(url, './tomato-detection.zip')\n\nwith zipfile.ZipFile('./tomato-detection.zip', 'r') as zip_ref:\n    zip_ref.extractall('.')\n\nclass TomatoDataset(Dataset):\n    def __init__(self, root, transforms):\n        self.root = root\n        self.transforms = transforms\n\n        self.images = list(sorted(os.listdir(os.path.join(root, 'images'))))\n        self.annotations = list(sorted(os.listdir(os.path.join(root, 'annotations'))))\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.root, \"images\", self.images[idx])\n        ann_path = os.path.join(self.root, \"annotations\", self.annotations[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        bboxes, labels = [], []\n        with open(ann_path, 'r') as f:\n            root = ET.parse(f).getroot()\n\n            for obj in root.iter('object'):\n                difficult = obj.find('difficult').text\n                if int(difficult) == 1:\n                    continue\n                cls_id = 1\n                xmlbox = obj.find('bndbox')\n                b = [float(xmlbox.find('xmin').text), float(xmlbox.find('ymin').text), float(xmlbox.find('xmax').text),\n                        float(xmlbox.find('ymax').text)]\n                bboxes.append(b)\n                labels.append(cls_id)\n\n        bboxes = torch.as_tensor(np.array(bboxes), dtype=torch.float32)\n        labels = torch.as_tensor(np.array(labels), dtype=torch.int64)\n\n        if self.transforms is not None:\n            res = self.transforms(image=np.array(img), bboxes=bboxes, class_labels=labels)\n\n        target = {\n            'boxes': [torch.Tensor(x) for x in res['bboxes']],\n            'labels': res['class_labels']\n        }\n        img = res['image']\n\n        return img, target\n\n    def __len__(self):\n        return len(self.images)\n\ndata_transforms = A.Compose([\n    A.Resize(height=256, width=256),\n    A.CenterCrop(height=224, width=224),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n\ndataset = TomatoDataset(root='./tomato-detection/data', transforms=data_transforms)\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset,\n                                                            [int(len(dataset)*0.9), len(dataset)-int(len(dataset)*0.9)],\n                                                            generator=torch.Generator().manual_seed(42))\ntest_dataset.transforms = A.Compose([ToTensorV2()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the dataset\n=====================\n\nLet\\'s see how our data looks like.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f'Number of training images: {len(train_dataset)}')\nprint(f'Number of test images: {len(test_dataset)}')\nprint(f'Example output of an image shape: {train_dataset[0][0].shape}')\nprint(f'Example output of a label: {train_dataset[0][1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading a Pre-trained Model\n===============================\n\nIn this tutorial, we will download a pre-trained SSDlite model and a\nMobileNetV3 Large backbone from the official PyTorch repository. For\nmore details, please refer to the [official\ndocumentation](https://pytorch.org/vision/stable/generated/torchvision.models.detection.ssdlite320_mobilenet_v3_large.html#torchvision.models.detection.ssdlite320_mobilenet_v3_large).\n\nAfter downloading the model, we will fine-tune it for our particular\nclasses. We will do it by replacing the pre-trained head with a new one\nthat matches our needs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from functools import partial\nfrom torch import nn\nimport torchvision\nfrom torchvision.models.detection import _utils as det_utils\nfrom torchvision.models.detection.ssdlite import SSDLiteClassificationHead\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nmodel = torchvision.models.detection.ssdlite320_mobilenet_v3_large(pretrained=True)\n\nin_channels = det_utils.retrieve_out_channels(model.backbone, (320, 320))\nnum_anchors = model.anchor_generator.num_anchors_per_location()\nnorm_layer = partial(nn.BatchNorm2d, eps=0.001, momentum=0.03)\n\nmodel.head.classification_head = SSDLiteClassificationHead(in_channels, num_anchors, 2, norm_layer)\n_ = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading Pre-trained Weights\n===========================\n\nFor this tutorial we will not include the training code itself, but will\ndownload and load pre-trained weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('./tomato-detection/ssd_model.pth'))\n_ = model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validating the Model With Deepchecks\n====================================\n\nNow, after we have the training data, test data and the model, we can\nvalidate the model with deepchecks test suites.\n\nImplementing the VisionData class\n---------------------------------\n\nThe checks in the package validate the model & data by calculating\nvarious quantities over the data, labels and predictions. In order to do\nthat, those must be in a pre-defined format, according to the task type.\nIn the following example we\\'re using pytorch. To see an implementation\nof this in tensorflow, please refer to\n`vision__vision_data_class`{.interpreted-text role=\"ref\"} guide. For\npytorch, we will use our DataLoader, but we\\'ll create a new collate\nfunction for it, that transforms the batch to the correct format. Then,\nwe\\'ll create a\n`deepchecks.vision.vision_data.vision_data.VisionData`{.interpreted-text\nrole=\"class\"} object, that will hold the data loader.\n\nTo learn more about the expected format please visit\n`vision__supported_tasks`{.interpreted-text role=\"ref\"}.\n\nFirst, we will create some functions that transform our batch to the\ncorrect format of images, labels and predictions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_untransformed_images(original_images):\n    \"\"\"\n    Convert a batch of data to images in the expected format. The expected format is an iterable of images,\n    where each image is a numpy array of shape (height, width, channels). The numbers in the array should be in the\n    range [0, 255] in a uint8 format.\n    \"\"\"\n    inp = torch.stack(list(original_images)).cpu().detach().numpy().transpose((0, 2, 3, 1))\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    # Un-normalize the images\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    return inp * 255\n\ndef transform_labels_to_cxywh(original_labels):\n    \"\"\"\n    Convert a batch of data to labels in the expected format. The expected format is an iterator of arrays, each array\n    corresponding to a sample. Each array element is in a shape of [B, 5], where B is the number of bboxes\n    in the image, and each bounding box is in the structure of [class_id, x, y, w, h].\n    \"\"\"\n    label = []\n    for annotation in original_labels:\n        if len(annotation[\"boxes\"]):\n            bbox = torch.stack(annotation[\"boxes\"])\n            # Convert the Pascal VOC xyxy format to xywh format\n            bbox[:, 2:] = bbox[:, 2:] - bbox[:, :2]\n            # The label shape is [class_id, x, y, w, h]\n            label.append(\n                torch.concat([torch.stack(annotation[\"labels\"]).reshape((-1, 1)), bbox], dim=1)\n            )\n        else:\n            # If it's an empty image, we need to add an empty label\n            label.append(torch.tensor([]))\n    return label\n\ndef infer_on_images(original_images):\n    \"\"\"\n    Returns the predictions for a batch of data. The expected format is an iterator of arrays, each array\n    corresponding to a sample. Each array element is in a shape of [B, 6], where B is the number of bboxes in the\n    predictions, and each bounding box is in the structure of [x, y, w, h, score, class_id].\n\n    Note that model and device here are global variables, and are defined in the previous code block, as the collate\n    function cannot recieve other arguments than the batch.\n    \"\"\"\n    nm_thrs = 0.2\n    score_thrs = 0.7\n    imgs = list(img.to(device) for img in original_images)\n    # Getting the predictions of the model on the batch\n    with torch.no_grad():\n        preds = model(imgs)\n    processed_pred = []\n    for pred in preds:\n        # Performoing non-maximum suppression on the detections\n        keep_boxes = torchvision.ops.nms(pred['boxes'], pred['scores'], nm_thrs)\n        score_filter = pred['scores'][keep_boxes] > score_thrs\n\n        # get the filtered result\n        test_boxes = pred['boxes'][keep_boxes][score_filter].reshape((-1, 4))\n        test_boxes[:, 2:] = test_boxes[:, 2:] - test_boxes[:, :2]  # xyxy to xywh\n        test_labels = pred['labels'][keep_boxes][score_filter]\n        test_scores = pred['scores'][keep_boxes][score_filter]\n\n        processed_pred.append(\n            torch.concat([test_boxes, test_scores.reshape((-1, 1)), test_labels.reshape((-1, 1))], dim=1))\n    return processed_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we\\'ll create the collate function that will be used by the\nDataLoader. In pytorch, the collate function is used to transform the\noutput batch to any custom format, and we\\'ll use that in order to\ntransform the batch to the correct format for the checks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.vision_data import BatchOutputFormat\n\ndef deepchecks_collate_fn(batch) -> BatchOutputFormat:\n    \"\"\"Return a batch of images, labels and predictions in the deepchecks format.\"\"\"\n    # batch received as iterable of tuples of (image, label) and transformed to tuple of iterables of images and labels:\n    batch = tuple(zip(*batch))\n    images = get_untransformed_images(batch[0])\n    labels = transform_labels_to_cxywh(batch[1])\n    predictions = infer_on_images(batch[0])\n    return BatchOutputFormat(images=images, labels=labels, predictions=predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have a single label here, which is the tomato class The label\\_map is\na dictionary that maps the class id to the class name, for display\npurposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "LABEL_MAP = {\n    1: 'Tomato'\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our updated collate function, we can recreate the\ndataloader in the deepchecks format, and use it to create a VisionData\nobject:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.vision_data import VisionData\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, collate_fn=deepchecks_collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=64, collate_fn=deepchecks_collate_fn)\n\ntraining_data = VisionData(batch_loader=train_loader, task_type='object_detection', label_map=LABEL_MAP)\ntest_data = VisionData(batch_loader=test_loader, task_type='object_detection', label_map=LABEL_MAP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Making sure our data is in the correct format:\n==============================================\n\nThe VisionData object automatically validates your data format and will\nalert you if there is a problem. However, you can also manually view\nyour images and labels to make sure they are in the correct format by\nusing the `head` function to conveniently visualize your data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "training_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running Deepchecks\\' suite on our data and model!\n=================================================\n\nNow that we have defined the task class, we can validate the model with\nthe deepchecks\\' model evaluation suite. This can be done with this\nsimple few lines of code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from deepchecks.vision.suites import model_evaluation\n\nsuite = model_evaluation()\nresult = suite.run(training_data, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also have suites for:\n`data integrity <deepchecks.vision.suites.data_integrity>`{.interpreted-text\nrole=\"func\"} - validating a single dataset and\n`train test validation <deepchecks.vision.suites.train_test_validation>`{.interpreted-text\nrole=\"func\"} -validating the dataset split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observing the results: {#observing_the_result}\n======================\n\nThe results can be saved as a html file with the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result.save_as_html('output.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or, if working inside a notebook, the output can be displayed directly\nby simply printing the result object:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that our model does not perform well, as can be seen in the\n\\\"Class Performance\\\" check under the \\\"Didn\\'t Pass\\\" section of the\nsuite results. This is because the model was trained on a different\ndataset, and the model was not trained to detect tomatoes. Moreover, we\ncan see that lowering the IoU threshold could have fixed this a bit (as\ncan be seen in the \\\"Mean Average Precision Report\\\" Check), but would\nstill keep the overall precision low. Moreover, under the \\\"Passed\\\"\nsection, we can see that our drift checks have passed, which means that\nthe distribution of the predictions on the training and test data is\nsimilar, and the issue is not there but in the model itself.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}